{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370e4a25-47e4-48b1-933d-1dd809a20bcd",
   "metadata": {},
   "source": [
    "## Encode Data To Model\n",
    "This is a cumulation of our data analysis and feature engineering.\n",
    "* Used to generate a .csv file that can be used directly to split for model testing.\n",
    "* Data we gathered from sql is striped of extra columns and formated in the way that the testing data was given so that we can replicate the steps when given the testing data. \n",
    "* The various features were tweaked, added, one hot encoded, binned and dropped as a result of testing on a baseline linear regression model and feature analysis. \n",
    "* Lastly the encoded data is saved as a .csv and stored in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "c6b2d1e8-7fcd-41df-8279-7ad7a6081d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import modules.help_functions as hf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "c5ba7cdd-fde7-4e21-b84e-67f168241ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lilakelland/Desktop/lighthouse_lab_midterm_project/src'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "dbb17ee8-0353-44f0-8975-d72d4dd314d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in files\n",
    "df = pd.read_csv('../../data/flights.csv')\n",
    "df_dep_delays = hf.get_avg_dep_delay(df, ['dep_delay'])\n",
    "df_dest_delays = hf.get_avg_dest_delay(df, ['carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "56c2ec26-f7f1-4d14-9f67-28182d3bbf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_format_to_test_data(df):\n",
    "    \"\"\" Convert our testing data to be in the same format as the data to test (drop columns and reformat date)\"\"\"\n",
    "    \n",
    "    #convert date to datetime with 0's\n",
    "    df.fl_date = (df.fl_date + ' 00:00:00')\n",
    "    pd.to_datetime(df['fl_date'])\n",
    "    \n",
    "    #drop columns not present in test format\n",
    "    df.drop(columns=['dep_time',\n",
    "       'dep_delay', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled',\n",
    "       'cancellation_code', 'diverted', 'actual_elapsed_time', 'air_time', \n",
    "       'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay',\n",
    "       'late_aircraft_delay', 'first_dep_time', 'total_add_gtime',\n",
    "       'longest_add_gtime','no_name'], inplace = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "f530f145-393c-4c98-b130-b111f684dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_engineering(df):\n",
    "    \"\"\"Adds in columns for model fitting and converts to numeric/ encoded categorical for ML model\"\"\"\n",
    "    \n",
    "    # Split crs_arr_time and crs_dep_time into hour of day (local)\n",
    "    df = hf.split_time_of_day_departure(df)\n",
    "    df = hf.split_time_of_day_arrival(df)\n",
    "    df.drop(columns=['crs_dep_time', 'crs_arr_time'], inplace=True)\n",
    "    \n",
    "    # encode hour of day departure\n",
    "    f = lambda x: str(int(np.floor(x)))\n",
    "\n",
    "    df['dep_hour_enc'] = df['dep_hour'].apply(f)\n",
    "    df['arr_hour_enc'] = df['arr_hour'].apply(f)\n",
    "    df.rename({'dep_hour_enc': 'dep_hour', 'arr_hour_enc': 'arr_hour'})\n",
    "    df = hf.encode_and_bind(df, 'dep_hour')\n",
    "    df = hf.encode_and_bind(df, 'arr_hour')\n",
    " \n",
    "    # Convert fl_date into day of week  \n",
    "    df = hf.add_weekday(df)\n",
    "    df.drop(columns=['fl_date'], inplace=True)\n",
    "    df = hf.encode_and_bind(df, 'weekday')\n",
    "    \n",
    "    # Add average delays (weather, carrier, NAS, departure)\n",
    "    df = df.merge(df_dest_delays, on='dest', how='left')\n",
    "    df = df.merge(df_dep_delays, on='origin', how='left')\n",
    "    \n",
    "    # Split city and state \n",
    "    hf.split_origin_city_state(df)\n",
    "    hf.split_dest_city_state(df)\n",
    "    df.drop(columns=['dest_city_name', 'origin_city_name'], inplace=True)\n",
    "    \n",
    "    # Encode top 10 cities in terms of traffic\n",
    "    city_list = ['Chicago','Atlanta','New York','Dallas/Fort Worth','Denver','Charlotte','Houston','Washington','Los Angeles','Seattle']    \n",
    "    df.dest_city = np.where(df.dest_city.isin(city_list),df.dest_city, '0')\n",
    "    df.origin_city = np.where(df.origin_city.isin(city_list),df.origin_city, '0')\n",
    "    df = hf.encode_and_bind(df, 'origin_city')\n",
    "    df = hf.encode_and_bind(df, 'dest_city')\n",
    "    df.drop(columns=['dest_city', 'origin_city'], inplace=True)\n",
    "    \n",
    "    #Top 20 airport codes - in terms of number of flights\n",
    "    top20_airport_code = ['LAX', 'ORD', 'EWR', 'SFO', 'LGA', 'DFW', 'LAS', 'CLT', 'DEN',\n",
    "                      'PHL', 'IAH', 'SEA', 'ATL', 'PHX', 'MCO', 'DTW', 'SLC', 'BOS',\n",
    "                      'JFK', 'MSP']\n",
    "    df.dest = np.where(df.dest.isin(top20_airport_code),df.dest, '0')\n",
    "    df.origin = np.where(df.origin.isin(top20_airport_code),df.origin, '0')\n",
    "    df = hf.encode_and_bind(df, 'dest')\n",
    "    df = hf.encode_and_bind(df, 'origin')\n",
    "    \n",
    "    # Remove negative targets (arr_delay - set to zero)  # kept this in here as it had an interesting effect - both MAE and R2 decreased\n",
    "#     df.arr_delay = np.where(df.arr_delay >0,df.arr_delay, 0)\n",
    "    \n",
    "    # State - Encode (based on # flights)\n",
    "    state_list = ['CA','TX', 'FL', 'IL', 'NY', 'GA', 'NC', 'CO', 'PA', 'WA']\n",
    "    df.dest_state = np.where(df.dest_state.isin(state_list),df.dest_state, '0')\n",
    "    df.origin_state = np.where(df.origin_state.isin(state_list),df.origin_state, '0')\n",
    "    df = hf.encode_and_bind(df, 'origin_state')\n",
    "    df = hf.encode_and_bind(df, 'dest_state')\n",
    "    df.drop(columns=['dest_state', 'origin_state'], inplace=True)\n",
    "          \n",
    "    # Convert Airline Carrier - Encode \n",
    "    df = hf.encode_and_bind(df, 'mkt_unique_carrier')\n",
    "    df.drop(columns = ['mkt_unique_carrier'], inplace=True)\n",
    "\n",
    "    # Origin Airport Bussiness - Encode top 10 (rest in 'other') OR BIN according to passenger or flight volume\n",
    "    df = hf.make_col_value_bins(df, 'origin', 'origin_airport_fl_amt_bin', 7) \n",
    "    \n",
    "    # Dest Airport Bussiness- Encode top 10 or bin according to passenger of flight volume \n",
    "    df = hf.make_col_value_bins(df, 'dest', 'dest_airport_fl_amt_bin', 7) \n",
    "   \n",
    "    # Flight number - drop for now\n",
    "    df.drop(columns = ['mkt_carrier_fl_num'], inplace=True)\n",
    "    \n",
    "    # crs_elapsed # USE LONG HAUL SHORT HAUL\n",
    "    df['log_crs_elapsed_time'] = np.log(df.crs_elapsed_time)\n",
    "    df = hf.make_bin_column(df, 'log_crs_elapsed_time', 20) \n",
    "    df = hf.make_categorical(df, ['log_crs_elapsed_time_bin'])\n",
    "    df = hf.encode_and_bind(df, 'log_crs_elapsed_time_bin')\n",
    "    df.drop(columns = ['crs_elapsed_time','log_crs_elapsed_time', 'log_crs_elapsed_time_bin' ], inplace=True)\n",
    "\n",
    "    # Drop rest\n",
    "    df.drop(columns=['branded_code_share', 'mkt_carrier','op_unique_carrier', 'tail_num', 'distance'\n",
    "                     'op_carrier_fl_num', 'dep_hour', 'origin_airport_id', 'arr_hour','dest_airport_id',  'dup', 'flights'], errors='ignore', inplace = True)\n",
    "\n",
    "    # Drop features from LDA \n",
    "    df.drop(columns=['origin_airport_fl_amt_bin', \n",
    "                'dest_airport_fl_amt_bin', \n",
    "                'dest_0', \n",
    "                'weekday_0', 'weekday_1', 'weekday_2', \n",
    "                'weekday_3', 'weekday_4', \n",
    "                'weekday_5', 'weekday_6', \n",
    "                'origin_city_0', \n",
    "                'origin_state_0'], errors='ignore', inplace = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "1d9b6a22-8164-4eff-b766-25cabfb70fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_na_outliers(df):\n",
    "    \"\"\" read data, convert to the format the given test data is in and add / format columns per feature engineering \"\"\"\n",
    "    # remove non-landing flights\n",
    "    df.dropna(subset=['arr_delay'], inplace=True)\n",
    "    \n",
    "    # remove outliers \n",
    "    cols = ['arr_delay'] \n",
    "    Q1 = df[cols].quantile(0.25)\n",
    "    Q3 = df[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ad6a6ab1-f54b-4d2d-b782-3f1846d42357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(df):\n",
    "    # scaler = MinMaxScaler()\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # apply scaler() to all the numeric columns \n",
    "    numeric_vars = ['avg_carrier_delay', \n",
    "                     'avg_weather_delay', \n",
    "                     'avg_nas_delay', \n",
    "                     'avg_security_delay', \n",
    "                     'avg_late_aircraft_delay', 'avg_dep_delay'\n",
    "                     ]\n",
    "    df[numeric_vars] = scaler.fit_transform(df[numeric_vars])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b11b98e7-84c2-490b-aad9-ab74ee3af0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOUND THAT THIS HAD NEGATIVE IMPACT AND NO LONGER USING\n",
    "#  def remove_small_variance(x, variance_threshold = 0.1):\n",
    "#     # Assumptions - target variable removed, df is numeric\n",
    "\n",
    "#     vt = VarianceThreshold(variance_threshold)\n",
    "#     x_transformed = vt.fit_transform(x)\n",
    "#     selected_columns = x.columns[vt.get_support()]\n",
    "#     x_transformed = pd.DataFrame(x_transformed, columns = selected_columns)\n",
    "#     return(x_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5e03a20d-8bc8-468b-b1a5-a12fa5c5e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(df, correlation_threshold=0.8):\n",
    "    \"\"\" Any pair above the correlation threshold, one feature will be removed \"\"\"\n",
    "    \n",
    "    df_corr = df.corr().abs()\n",
    "\n",
    "    indices = np.where(df_corr > correlation_threshold)\n",
    "    indices = [(df_corr.index[x], df_corr.columns[y])\n",
    "    for x, y in zip(*indices)\n",
    "        if x != y and x < y]\n",
    "\n",
    "    for idx in indices: #each pair\n",
    "        try:\n",
    "            df.drop(idx[1], axis = 1, inplace=True)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0b9717b0-5c5b-4b5e-862b-40e074e27b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform df to encoded training data\n",
    "# df = pd.read_csv('../data/flights.csv')\n",
    "# df_dep_delays = hf.get_avg_dep_delay(df, ['dep_delay'])\n",
    "# df_dest_delays = hf.get_avg_dest_delay(df, ['carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay'])\n",
    "\n",
    "df = change_format_to_test_data(df)\n",
    "df = add_feature_engineering(df)\n",
    "df = data_to_model_format(df)\n",
    "df = remove_na_outliers(df)\n",
    "df = scale(df)\n",
    "df = remove_highly_correlated_features(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "498ed1be-239d-46f3-8629-0829abf7426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to data folder\n",
    "df.to_csv(\"../../data/encoded_training_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b7822-5936-4e68-ac1e-d0ae755892ed",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "a382e85f-7e09-4cc7-bf63-a3d5b80a46b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " r2: 0.07588064956536089\n",
      " MSE: 271.8742627386297\n",
      " MAE: 12.848907195090016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "y = df.arr_delay.to_numpy()\n",
    "X = df.drop(columns=['arr_delay']).to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=88)\n",
    "\n",
    "lr_baseline = LinearRegression()\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Save pickle file\n",
    "# model = lr_baseline\n",
    "# filename = '../model/linear_regression_all_features_except_neg_target.pkl'\n",
    "# pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "y_pred = lr_baseline.predict(X_test)\n",
    "\n",
    "r2_baseline = r2_score(y_test, y_pred)\n",
    "MSE_baseline = mean_squared_error(y_test,y_pred) \n",
    "RMSE_baseline = mean_squared_error(y_test,y_pred,squared=False)\n",
    "MAE_baseline = mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "print(f' r2: {r2_baseline}\\n MSE: {MSE_baseline}\\n MAE: {MAE_baseline}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d2afef",
   "metadata": {},
   "outputs": [],
   "source": [
    " r2: 0.08013667354836995\n",
    " MSE: 270.622148082623\n",
    " MAE: 12.81701225015653"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
