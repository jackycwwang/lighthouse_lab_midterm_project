{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3726db75-b88b-4423-a1b7-fc1a44ec1112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import help_functions as hf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMAxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837044b-b0ed-43aa-b093-87fb5153b5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c6acfbd-4110-4d9a-b821-7239dc60d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/flights_samp.csv')\n",
    "\n",
    "def convert_testtrain_data_to_test_format(df):\n",
    "    \"\"\" Convert our testing data to be in the same format as the data to test (drop columns and reformat date)\"\"\"\n",
    "    \n",
    "    #convert date to datetime with 0's\n",
    "    df.fl_date = (df.fl_date + ' 00:00:00')\n",
    "    pd.to_datetime(df['fl_date'])\n",
    "    \n",
    "    #drop columns not present in test format\n",
    "    df.drop(columns=['index', 'dep_time',\n",
    "       'dep_delay', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'arr_delay', 'cancelled',\n",
    "       'cancellation_code', 'diverted', 'actual_elapsed_time', 'air_time', \n",
    "       'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay',\n",
    "       'late_aircraft_delay', 'first_dep_time', 'total_add_gtime',\n",
    "       'longest_add_gtime', 'no_name'], inplace = True)\n",
    "    return df\n",
    "# df_new = convert_testtrain_data_to_test_format(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46051c94-d76e-4466-a2bb-b7bec4126f8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# THIS WILL BE CHANGED AS WE FEATURE ENGINEER\n",
    "\n",
    "def convert_from_test_format_to_fit_predict_format(df):\n",
    "    \"\"\"Adds in columns for model fitting and converts to numeric/ encoded categorical for ML model\"\"\"\n",
    "    \n",
    "    # Split crs_arr_time and crs_dep_time into hour of day (local)\n",
    "    df = hf.split_time_of_day_departure(df)\n",
    "    df = hf.split_time_of_day_arrival(df)\n",
    "    df.drop(columns=['crs_dep_time', 'crs_arr_time'], inplace=True)\n",
    "    \n",
    "    # Convert fl_date into day of week  # NOTE MAY WANT TO ADD BACK IN MONTH OR JAN 1 days\n",
    "    df = hf.add_weekday(df)  \n",
    "    df.drop(columns=['fl_date'], inplace=True)\n",
    "    df.weekday = df.weekday.astype(str)\n",
    "    df = hf.encode_and_bind(df, 'weekday')\n",
    "    \n",
    "    # Split city and state \n",
    "    hf.split_origin_city_state(df)\n",
    "    hf.split_dest_city_state(df)\n",
    "    df.drop(columns=['dest_city_name', 'origin_city_name'], inplace=True)\n",
    "    \n",
    "    # City - Encode (based on # flights)\n",
    "    city_dict = {'Chicago': 10,\n",
    "             'Atlanta': 9,\n",
    "             'New York': 8,\n",
    "             'Dallas/Fort Worth': 7,\n",
    "             'Denver': 6,\n",
    "             'Charlotte': 5,\n",
    "             'Houston': 4,\n",
    "             'Washington': 3,\n",
    "             'Los Angeles': 2,\n",
    "             'Seattle': 1}\n",
    "    df = df.replace({\"dest_city\":city_dict})\n",
    "    df = df.replace({\"origin_city\":city_dict})\n",
    "    \n",
    "    city_list = [10,9,8,7,6,5,4,3,2,1]\n",
    "    df.dest_city = np.where(df.dest_city.isin(city_list),df.dest_city, 0)\n",
    "    df.origin_city = np.where(df.origin_city.isin(city_list),df.origin_city, 0)\n",
    "    \n",
    "    # State - Encode (based on # flights)\n",
    "    state_dict = {'CA': 10,\n",
    "                 'TX': 9,\n",
    "                 'FL': 8,\n",
    "                 'IL': 7,\n",
    "                 'NY': 6,\n",
    "                 'GA': 5,\n",
    "                 'NC': 4,\n",
    "                 'CO': 3,\n",
    "                 'PA': 2,\n",
    "                 'WA': 1}\n",
    "\n",
    "    df = df.replace({\"dest_state\":state_dict})\n",
    "    df = df.replace({\"origin_state\":state_dict})\n",
    "    \n",
    "    state_list = [10,9,8,7,6,5,4,3,2,1]\n",
    "    df.dest_state = np.where(df.dest_state.isin(state_list),df.dest_state, 0)\n",
    "    df.origin_state = np.where(df.origin_state.isin(state_list),df.origin_state, 0)\n",
    "          \n",
    "    # Convert Carrier - Encode \n",
    "    df = hf.encode_and_bind(df, 'mkt_unique_carrier')\n",
    "    df.drop(columns = ['mkt_unique_carrier'], inplace=True)\n",
    "\n",
    "    # Origin Airport - Encode top 10 (rest in 'other') OR BIN according to passenger or flight volume\n",
    "    df = hf.make_col_value_bins(df, 'origin', 'origin_airport_fl_amt_bin', 7) \n",
    "    \n",
    "    # Dest Airport - Encode top 10 or bin according to passenger of flight volume \n",
    "    df = hf.make_col_value_bins(df, 'dest', 'dest_airport_fl_amt_bin', 7) \n",
    "   \n",
    "    # Flight number ??? # drop for now? - See if routes that have more delays? Bin when find relationship with delays\n",
    "    df.drop(columns = ['mkt_carrier_fl_num'], inplace=True)\n",
    "    \n",
    "    # Drop rest\n",
    "    df.drop(columns=['branded_code_share', 'mkt_carrier','op_unique_carrier', 'tail_num', \n",
    "                     'op_carrier_fl_num', 'origin_airport_id', 'dest_airport_id', 'dup', 'flights'], inplace = True)\n",
    "    \n",
    "    # crs_elapsed # USE LONG HAUL SHORT HAUL\n",
    "    df['log_crs_elapsed_time'] = np.log(df.crs_elapsed_time)\n",
    "    df = hf.make_bin_column(df, 'log_crs_elapsed_time', 20) # 8-2/0.3\n",
    "    df.drop(columns = ['crs_elapsed_time'], inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "    #/ distance choose one or other - correlated removal?\n",
    "\n",
    "# df_new = convert_from_test_format_to_fit_predict_format(df_new)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b592e-0beb-40bf-be18-08854ec15830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    \"\"\" read in data file, convert to the format the given test data is in and add / format columns per feature engineering \"\"\"\n",
    "    df = pd.read_csv('../data/flights_samp.csv')\n",
    "    df = convert_testtrain_data_to_test_format(df)\n",
    "    df = convert_from_test_format_to_fit_predict_format(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5f4cdc-291d-480f-acc3-4d03431c7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "    df = remove_highly_correlated_features(df, correlation_threshold=0.8)\n",
    "    df = remove_small_variance(x, variance_threshold = 0.1)\n",
    "#     SCALE DATA\n",
    "\n",
    "def remove_highly_correlated_features(df, correlation_threshold=0.8):\n",
    "    #     Anything above correlation threshold will be tossed\n",
    "    # Assumptions - all numeric, target variable removed\n",
    "    # step 1\n",
    "    df_corr = df.corr().abs()\n",
    "\n",
    "    # step 2\n",
    "    indices = np.where(df_corr > correlation_threshold)\n",
    "    indices = [(df_corr.index[x], df_corr.columns[y])\n",
    "    for x, y in zip(*indices)\n",
    "        if x != y and x < y]\n",
    "\n",
    "    # step 3\n",
    "    for idx in indices: #each pair\n",
    "        try:\n",
    "            df.drop(idx[1], axis = 1, inplace=True)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return(df)\n",
    "\n",
    " \n",
    "def remove_small_variance(x, variance_threshold = 0.1):\n",
    "    # Assumptions - target variable removed, df is numeric\n",
    "    # import:\n",
    "    # from sklearn.feature_selection import VarianceThreshold\n",
    "    vt = VarianceThreshold(variance_threshold)\n",
    "    x_transformed = vt.fit_transform(x)\n",
    "    selected_columns = x.columns[vt.get_support()]\n",
    "    x_transformed = pd.DataFrame(x_transformed, columns = selected_columns)\n",
    "    return(x_transformed)\n",
    "\n",
    "def remove_missing_values(x, missing_percent_drop_threshold=0.5):\n",
    "#     takes in dataframe, removes missing above a percent threshold - percent out of 1\n",
    "    total = x.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (x.isnull().sum()/x.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    missing_data.head(20)\n",
    "\n",
    "    to_drop = missing_data[missing_data['Percent'] > missing_percent_drop_threshold].index.tolist()\n",
    "    return(x.drop(to_drop, axis=1, inplace=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
