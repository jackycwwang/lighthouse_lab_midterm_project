{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d762ca93-c9bb-4d0a-8e0c-3d60c23cfcd6",
   "metadata": {},
   "source": [
    "## Baseline Modelling - Linear Regression \n",
    "\n",
    "Started with the features from original dataframe - in test format, then\n",
    "label encoded, qcut binned and one hot encoded\n",
    "\n",
    "No scaling or additional features where added on initial run. r2: 0.017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6b2d1e8-7fcd-41df-8279-7ad7a6081d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import modules.help_functions as hf\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56c2ec26-f7f1-4d14-9f67-28182d3bbf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_testtrain_data_to_test_format(df):\n",
    "    \"\"\" Convert our testing data to be in the same format as the data to test (drop columns and reformat date)\"\"\"\n",
    "    \n",
    "    #convert date to datetime with 0's\n",
    "    df.fl_date = (df.fl_date + ' 00:00:00')\n",
    "    pd.to_datetime(df['fl_date'])\n",
    "    \n",
    "    #drop columns not present in test format\n",
    "    df.drop(columns=['dep_time',\n",
    "       'dep_delay', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled',\n",
    "       'cancellation_code', 'diverted', 'actual_elapsed_time', 'air_time', \n",
    "       'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay',\n",
    "       'late_aircraft_delay', 'first_dep_time', 'total_add_gtime',\n",
    "       'longest_add_gtime','no_name'], inplace = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f530f145-393c-4c98-b130-b111f684dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_test_format_to_fit_predict_format(df):\n",
    "    \"\"\"Adds in columns for model fitting and converts to numeric/ encoded categorical for ML model\"\"\"\n",
    "    \n",
    "    # Split crs_arr_time and crs_dep_time into hour of day (local)\n",
    "    df = hf.split_time_of_day_departure(df)\n",
    "    df = hf.split_time_of_day_arrival(df)\n",
    "    df.drop(columns=['crs_dep_time', 'crs_arr_time'], inplace=True)\n",
    "    \n",
    "    # Convert fl_date into day of week  # NOTE MAY WANT TO ADD BACK IN MONTH OR JAN 1 days\n",
    "\n",
    "#     df = hf.add_weekday(df)  \n",
    "    df.drop(columns=['fl_date'], inplace=True)\n",
    "#     df = hf.encode_and_bind(df, 'weekday')\n",
    "    \n",
    "    # Split city and state \n",
    "    hf.split_origin_city_state(df)\n",
    "    hf.split_dest_city_state(df)\n",
    "    df.drop(columns=['dest_city_name', 'origin_city_name'], inplace=True)\n",
    "    \n",
    "    # City - Encode (based on # flights)\n",
    "    city_dict = {'Chicago': 10,\n",
    "             'Atlanta': 9,\n",
    "             'New York': 8,\n",
    "             'Dallas/Fort Worth': 7,\n",
    "             'Denver': 6,\n",
    "             'Charlotte': 5,\n",
    "             'Houston': 4,\n",
    "             'Washington': 3,\n",
    "             'Los Angeles': 2,\n",
    "             'Seattle': 1}\n",
    "    df = df.replace({\"dest_city\":city_dict})\n",
    "    df = df.replace({\"origin_city\":city_dict})\n",
    "    \n",
    "    city_list = [10,9,8,7,6,5,4,3,2,1]\n",
    "    df.dest_city = np.where(df.dest_city.isin(city_list),df.dest_city, 0)\n",
    "    df.origin_city = np.where(df.origin_city.isin(city_list),df.origin_city, 0)\n",
    "    \n",
    "    # State - Encode (based on # flights)\n",
    "    state_dict = {'CA': 10,\n",
    "                 'TX': 9,\n",
    "                 'FL': 8,\n",
    "                 'IL': 7,\n",
    "                 'NY': 6,\n",
    "                 'GA': 5,\n",
    "                 'NC': 4,\n",
    "                 'CO': 3,\n",
    "                 'PA': 2,\n",
    "                 'WA': 1}\n",
    "\n",
    "    df = df.replace({\"dest_state\":state_dict})\n",
    "    df = df.replace({\"origin_state\":state_dict})\n",
    "    \n",
    "    state_list = [10,9,8,7,6,5,4,3,2,1]\n",
    "    df.dest_state = np.where(df.dest_state.isin(state_list),df.dest_state, 0)\n",
    "    df.origin_state = np.where(df.origin_state.isin(state_list),df.origin_state, 0)\n",
    "          \n",
    "    # Convert Carrier - Encode \n",
    "    df = hf.encode_and_bind(df, 'mkt_unique_carrier')\n",
    "    df.drop(columns = ['mkt_unique_carrier'], inplace=True)\n",
    "\n",
    "    # Origin Airport - Encode top 10 (rest in 'other') OR BIN according to passenger or flight volume\n",
    "    df = hf.make_col_value_bins(df, 'origin', 'origin_airport_fl_amt_bin', 7) \n",
    "    \n",
    "    # Dest Airport - Encode top 10 or bin according to passenger of flight volume \n",
    "    df = hf.make_col_value_bins(df, 'dest', 'dest_airport_fl_amt_bin', 7) \n",
    "   \n",
    "    # Flight number ??? # drop for now? - See if routes that have more delays? Bin when find relationship with delays\n",
    "    df.drop(columns = ['mkt_carrier_fl_num'], inplace=True)\n",
    "    \n",
    "    # Drop rest\n",
    "    df.drop(columns=['branded_code_share', 'mkt_carrier','op_unique_carrier', 'tail_num', \n",
    "                     'op_carrier_fl_num', 'origin_airport_id', 'dest_airport_id',  'dup', 'flights'], inplace = True)\n",
    "    \n",
    "    # crs_elapsed # USE LONG HAUL SHORT HAUL\n",
    "    df['log_crs_elapsed_time'] = np.log(df.crs_elapsed_time)\n",
    "    df = hf.make_bin_column(df, 'log_crs_elapsed_time', 20) # 8-2/0.3\n",
    "    df.drop(columns = ['crs_elapsed_time','log_crs_elapsed_time' ], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d9b6a22-8164-4eff-b766-25cabfb70fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    \"\"\" read in data file, convert to the format the given test data is in and add / format columns per feature engineering \"\"\"\n",
    "   \n",
    "    # read in file\n",
    "    df = pd.read_csv('../data/flights.csv')\n",
    "    \n",
    "    # remove non-landing flights\n",
    "    df.dropna(subset=['arr_delay'], inplace=True)\n",
    "    \n",
    "#     remove outliers ### IS THIS GOING TO BACKFIRE?\n",
    "#     cols = ['arr_delay'] \n",
    "#     Q1 = df[cols].quantile(0.25)\n",
    "#     Q3 = df[cols].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     df = df[~((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    \n",
    "    # convert\n",
    "    df = convert_testtrain_data_to_test_format(df)\n",
    "    df = convert_from_test_format_to_fit_predict_format(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57802728-4cd6-4a5e-8bdd-893e53d5b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41272dcb-b8d6-4b54-8fff-f3a10c1505ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e23285e9-5b67-45c6-b17d-51ae8161bbb5",
   "metadata": {},
   "source": [
    "### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad6a6ab1-f54b-4d2d-b782-3f1846d42357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rescale the features\n",
    "# scaler = MinMaxScaler()\n",
    "# # scalar = StandardScaler()\n",
    "\n",
    "# # apply scaler() to all the numeric columns \n",
    "# numeric_vars = ['distance', 'dep_hour', 'arr_hour', 'dep_hour']\n",
    "# df[numeric_vars] = scaler.fit_transform(df[numeric_vars])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb32053-5a65-4012-9be4-0ea143809e7d",
   "metadata": {},
   "source": [
    "### Remove Highly Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b98e7-84c2-490b-aad9-ab74ee3af0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def remove_small_variance(x, variance_threshold = 0.1):\n",
    "#     # Assumptions - target variable removed, df is numeric\n",
    "#     # import:\n",
    "#     # from sklearn.feature_selection import VarianceThreshold\n",
    "#     vt = VarianceThreshold(variance_threshold)\n",
    "#     x_transformed = vt.fit_transform(x)\n",
    "#     selected_columns = x.columns[vt.get_support()]\n",
    "#     x_transformed = pd.DataFrame(x_transformed, columns = selected_columns)\n",
    "#     return(x_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef28f4b-8d52-4936-9f0a-acbc65e26711",
   "metadata": {},
   "source": [
    "#### Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a563196-be6e-4348-8666-3884c5d82bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.arr_delay.to_numpy()\n",
    "X = df.drop(columns=['arr_delay']).to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0449eed9-64cc-41a3-9075-dba3ddf1c7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280574, 21)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22eb60a2-cb07-41c5-bfbf-2ca5872f7bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70144, 21)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03a20d-8bc8-468b-b1a5-a12fa5c5e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_highly_correlated_features(df, correlation_threshold=0.8):\n",
    "#     #     Anything above correlation threshold will be tossed\n",
    "#     # Assumptions - all numeric, target variable removed\n",
    "#     # step 1\n",
    "#     df_corr = df.corr().abs()\n",
    "\n",
    "#     # step 2\n",
    "#     indices = np.where(df_corr > correlation_threshold)\n",
    "#     indices = [(df_corr.index[x], df_corr.columns[y])\n",
    "#     for x, y in zip(*indices)\n",
    "#         if x != y and x < y]\n",
    "\n",
    "#     # step 3\n",
    "#     for idx in indices: #each pair\n",
    "#         try:\n",
    "#             df.drop(idx[1], axis = 1, inplace=True)\n",
    "#         except KeyError:\n",
    "#             pass\n",
    "#     return(df)\n",
    "\n",
    "# X= remove_highly_correlated_features(X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b7822-5936-4e68-ac1e-d0ae755892ed",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a382e85f-7e09-4cc7-bf63-a3d5b80a46b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " r2: 0.01707328893248994\n",
      " MSE: 2269.260375035167\n",
      " MAE: 24.096536481839905\n",
      " model_fit 0.01707328893248994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "lr_baseline = LinearRegression()\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Save pickle file\n",
    "model = lr_baseline\n",
    "filename = '../model/lr_baseline.pkl'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "y_pred = lr_baseline.predict(X_test)\n",
    "\n",
    "r2_baseline = r2_score(y_test, y_pred)\n",
    "MSE_baseline = mean_squared_error(y_test,y_pred) \n",
    "RMSE_baseline = mean_squared_error(y_test,y_pred,squared=False)\n",
    "MAE_baseline = mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "print(f' r2: {r2_baseline}\\n MSE: {MSE_baseline}\\n MAE: {MAE_baseline}\\n model_fit {r2_baseline}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e238085-2b53-4731-93dc-f02fac6aa940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01679509, 0.01412412, 0.01425259, 0.01582781, 0.01535879])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # create a KFold object with 5 splits \n",
    "lm = LinearRegression()\n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "scores = cross_val_score(lm, X_train, y_train, scoring='r2', cv=folds)\n",
    "scores   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "014bf327-fe6c-461e-a025-836a275b4331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683574fc-1cc2-43ab-b996-05d23f82d929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514a460-c97f-43d1-9f5d-72f4be07e704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad1804-21e8-4a09-ad09-19aaae376a63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
