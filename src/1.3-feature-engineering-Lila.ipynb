{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d762ca93-c9bb-4d0a-8e0c-3d60c23cfcd6",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b2d1e8-7fcd-41df-8279-7ad7a6081d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import modules.help_functions as hf\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb17ee8-0353-44f0-8975-d72d4dd314d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file\n",
    "df = pd.read_csv('../data/flights.csv')\n",
    "df_delays = hf.get_avg_delay(df, ['carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c2ec26-f7f1-4d14-9f67-28182d3bbf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_testtrain_data_to_test_format(df):\n",
    "    \"\"\" Convert our testing data to be in the same format as the data to test (drop columns and reformat date)\"\"\"\n",
    "    \n",
    "    #convert date to datetime with 0's\n",
    "    df.fl_date = (df.fl_date + ' 00:00:00')\n",
    "    pd.to_datetime(df['fl_date'])\n",
    "    \n",
    "    #drop columns not present in test format\n",
    "    df.drop(columns=['dep_time',\n",
    "       'dep_delay', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled',\n",
    "       'cancellation_code', 'diverted', 'actual_elapsed_time', 'air_time', \n",
    "       'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay',\n",
    "       'late_aircraft_delay', 'first_dep_time', 'total_add_gtime',\n",
    "       'longest_add_gtime','no_name'], inplace = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "898f8223-bffb-443d-ac99-32fa4c497338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f530f145-393c-4c98-b130-b111f684dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_test_format_to_fit_predict_format(df):\n",
    "    \"\"\"Adds in columns for model fitting and converts to numeric/ encoded categorical for ML model\"\"\"\n",
    "    \n",
    "    # Split crs_arr_time and crs_dep_time into hour of day (local)\n",
    "    df = hf.split_time_of_day_departure(df)\n",
    "    df = hf.split_time_of_day_arrival(df)\n",
    "    df.drop(columns=['crs_dep_time', 'crs_arr_time'], inplace=True)\n",
    "    \n",
    "    #take log distance\n",
    "    df['distance_log'] = np.log(df.distance)\n",
    "    df.drop(columns=['distance'], inplace=True)\n",
    "    \n",
    "    # encode hour of day departure\n",
    "    f = lambda x: str(int(np.floor(x)))\n",
    "    df['dep_hour'] = df['dep_hour'].apply(f)\n",
    "    df['arr_hour'] = df['arr_hour'].apply(f)\n",
    "    df = hf.encode_and_bind(df, 'dep_hour')\n",
    "    df = hf.encode_and_bind(df, 'arr_hour')\n",
    " \n",
    "    # Convert fl_date into day of week  # NOTE MAY WANT TO ADD BACK IN MONTH OR JAN 1 days\n",
    "    df = hf.add_weekday(df)  \n",
    "    df.drop(columns=['fl_date'], inplace=True)\n",
    "    df = hf.encode_and_bind(df, 'weekday')\n",
    "    \n",
    "    # Add average delays to dest\n",
    "    df = df.merge(df_delays, on='dest', how='left')\n",
    "    \n",
    "    # Split city and state \n",
    "    hf.split_origin_city_state(df)\n",
    "    hf.split_dest_city_state(df)\n",
    "    df.drop(columns=['dest_city_name', 'origin_city_name'], inplace=True)\n",
    "    \n",
    "    # Encode top 10 cities in terms of traffic\n",
    "    city_list = ['Chicago','Atlanta','New York','Dallas/Fort Worth','Denver','Charlotte','Houston','Washington','Los Angeles','Seattle']\n",
    "    df.dest_city = np.where(df.dest_city.isin(city_list),df.dest_city, '0')\n",
    "    df.origin_city = np.where(df.origin_city.isin(city_list),df.origin_city, '0')\n",
    "    df = hf.encode_and_bind(df, 'origin_city')\n",
    "    df = hf.encode_and_bind(df, 'dest_city')\n",
    "    df.drop(columns=['dest_city', 'origin_city'], inplace=True)\n",
    "    \n",
    "    #Top 20 airport codes\n",
    "    top20_airport_code = ['LAX', 'ORD', 'EWR', 'SFO', 'LGA', 'DFW', 'LAS', 'CLT', 'DEN',\n",
    "                      'PHL', 'IAH', 'SEA', 'ATL', 'PHX', 'MCO', 'DTW', 'SLC', 'BOS',\n",
    "                      'JFK', 'MSP']\n",
    "    df.dest = np.where(df.dest.isin(top20_airport_code),df.dest, '0')\n",
    "    df.origin = np.where(df.origin.isin(top20_airport_code),df.origin, '0')\n",
    "    df = hf.encode_and_bind(df, 'dest')\n",
    "    df = hf.encode_and_bind(df, 'origin')\n",
    "    \n",
    "    #REMOVE negative targets (arr_delay - set to zero)\n",
    "#     df.arr_delay = np.where(df.arr_delay >0,df.arr_delay, 0)\n",
    "    \n",
    "    # State - Encode (based on # flights)\n",
    "    state_list = ['CA','TX', 'FL', 'IL', 'NY', 'GA', 'NC', 'CO', 'PA', 'WA']\n",
    "    df.dest_state = np.where(df.dest_state.isin(state_list),df.dest_state, '0')\n",
    "    df.origin_state = np.where(df.origin_state.isin(state_list),df.origin_state, '0')\n",
    "    df = hf.encode_and_bind(df, 'origin_state')\n",
    "    df = hf.encode_and_bind(df, 'dest_state')\n",
    "    df.drop(columns=['dest_state', 'origin_state'], inplace=True)\n",
    "          \n",
    "    # Convert Airline Carrier - Encode \n",
    "    df = hf.encode_and_bind(df, 'mkt_unique_carrier')\n",
    "    df.drop(columns = ['mkt_unique_carrier'], inplace=True)\n",
    "\n",
    "    # Origin Airport - Encode top 10 (rest in 'other') OR BIN according to passenger or flight volume\n",
    "    df = hf.make_col_value_bins(df, 'origin', 'origin_airport_fl_amt_bin', 7) \n",
    "    \n",
    "    \n",
    "    # Dest Airport - Encode top 10 or bin according to passenger of flight volume \n",
    "    df = hf.make_col_value_bins(df, 'dest', 'dest_airport_fl_amt_bin', 7) \n",
    "   \n",
    "    # Flight number ??? # drop for now? \n",
    "    df.drop(columns = ['mkt_carrier_fl_num'], inplace=True)\n",
    "    \n",
    "    # crs_elapsed # USE LONG HAUL SHORT HAUL\n",
    "    df['log_crs_elapsed_time'] = np.log(df.crs_elapsed_time)\n",
    "    df = hf.make_bin_column(df, 'log_crs_elapsed_time', 10) # 8-2/0.3\n",
    "    df.log_crs_elapsed_time_bin = df.log_crs_elapsed_time_bin.replace({1:'a', 2: 'b',3:'c',4:'d', 5:'e', 6:'f', 7:'g',8:'h', 9:'i', 10:'j'}) \n",
    "    df = hf.encode_and_bind(df, 'log_crs_elapsed_time_bin')\n",
    "    df.drop(columns = ['crs_elapsed_time','log_crs_elapsed_time', 'log_crs_elapsed_time_bin' ], inplace=True)\n",
    "\n",
    "    # Drop rest\n",
    "    df.drop(columns=['branded_code_share', 'mkt_carrier','op_unique_carrier', 'tail_num', \n",
    "                     'op_carrier_fl_num', 'dep_hour', 'origin_airport_id', 'arr_hour','dest_airport_id',  'dup', 'flights'], errors='ignore', inplace = True)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d9b6a22-8164-4eff-b766-25cabfb70fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    \"\"\" read in data file, convert to the format the given test data is in and add / format columns per feature engineering \"\"\"\n",
    "   \n",
    "    df = pd.read_csv('../data/flights.csv')\n",
    "    \n",
    "    # remove non-landing flights\n",
    "    df.dropna(subset=['arr_delay'], inplace=True)\n",
    "    \n",
    "#     remove outliers ### IS THIS GOING TO BACKFIRE?\n",
    "#     df_rm = df_rm[(np.abs(stats.zscore(df_rm['arr_delay'])) < 3)]\n",
    "    cols = ['arr_delay'] \n",
    "    Q1 = df[cols].quantile(0.25)\n",
    "    Q3 = df[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    \n",
    "    # convert\n",
    "    df = convert_testtrain_data_to_test_format(df)\n",
    "    df = convert_from_test_format_to_fit_predict_format(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57802728-4cd6-4a5e-8bdd-893e53d5b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23285e9-5b67-45c6-b17d-51ae8161bbb5",
   "metadata": {},
   "source": [
    "### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a7e8fb6-fe7b-4d47-981a-70c3bab74812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad6a6ab1-f54b-4d2d-b782-3f1846d42357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the features\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# apply scaler() to all the numeric columns \n",
    "numeric_vars = ['avg_carrier_delay', \n",
    "                 'avg_weather_delay', \n",
    "                 'avg_nas_delay', \n",
    "                 'avg_security_delay', \n",
    "                 'avg_late_aircraft_delay', 'distance_log'\n",
    "                 ]\n",
    "df[numeric_vars] = scaler.fit_transform(df[numeric_vars])\n",
    "# list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb32053-5a65-4012-9be4-0ea143809e7d",
   "metadata": {},
   "source": [
    "### Remove Highly Correlated Features and small Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b11b98e7-84c2-490b-aad9-ab74ee3af0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    " def remove_small_variance(x, variance_threshold = 0.1):\n",
    "    # Assumptions - target variable removed, df is numeric\n",
    "    # import:\n",
    "    # from sklearn.feature_selection import VarianceThreshold\n",
    "    vt = VarianceThreshold(variance_threshold)\n",
    "    x_transformed = vt.fit_transform(x)\n",
    "    selected_columns = x.columns[vt.get_support()]\n",
    "    x_transformed = pd.DataFrame(x_transformed, columns = selected_columns)\n",
    "    return(x_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e03a20d-8bc8-468b-b1a5-a12fa5c5e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(df, correlation_threshold=0.8):\n",
    "    #     Anything above correlation threshold will be tossed\n",
    "    # Assumptions - all numeric, target variable removed\n",
    "    # step 1\n",
    "    df_corr = df.corr().abs()\n",
    "\n",
    "    # step 2\n",
    "    indices = np.where(df_corr > correlation_threshold)\n",
    "    indices = [(df_corr.index[x], df_corr.columns[y])\n",
    "    for x, y in zip(*indices)\n",
    "        if x != y and x < y]\n",
    "\n",
    "    # step 3\n",
    "    for idx in indices: #each pair\n",
    "        try:\n",
    "            df.drop(idx[1], axis = 1, inplace=True)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b7822-5936-4e68-ac1e-d0ae755892ed",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a382e85f-7e09-4cc7-bf63-a3d5b80a46b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " r2: 0.07715684217044894\n",
      " MSE: 271.49880915305374\n",
      " MAE: 12.843306393864719\n",
      " model_fit 0.07715684217044894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "y = df.arr_delay.to_numpy()\n",
    "X = df.drop(columns=['arr_delay']).to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=88)\n",
    "\n",
    "lr_baseline = LinearRegression()\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "\n",
    "# # Save pickle file\n",
    "# model = lr_baseline\n",
    "# filename = '../model/linear_regression_all_features_except_neg_target.pkl'\n",
    "# pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "y_pred = lr_baseline.predict(X_test)\n",
    "\n",
    "r2_baseline = r2_score(y_test, y_pred)\n",
    "MSE_baseline = mean_squared_error(y_test,y_pred) \n",
    "RMSE_baseline = mean_squared_error(y_test,y_pred,squared=False)\n",
    "MAE_baseline = mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "print(f' r2: {r2_baseline}\\n MSE: {MSE_baseline}\\n MAE: {MAE_baseline}\\n model_fit {r2_baseline}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407082f2-a620-4edb-b437-c83c32829b10",
   "metadata": {},
   "source": [
    "## Polynomial Fit 1-3 degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c865d7c4-f1ef-407a-ac52-e159eea9a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "degrees = [1, 2, 3]\n",
    "\n",
    "# initialise y_train_pred and y_test_pred matrices to store the train and test predictions\n",
    "# each row is a data point, each column a prediction using a polynomial of some degree\n",
    "y_train_pred = np.zeros((len(X_train), len(degrees)))\n",
    "y_test_pred = np.zeros((len(X_test), len(degrees)))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    \n",
    "    # make pipeline: create features, then feed them to linear_reg model\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # predict on test and train data\n",
    "    # store the predictions of each degree in the corresponding column\n",
    "    y_train_pred[:, i] = model.predict(X_train)\n",
    "    y_test_pred[:, i] = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f24f0d4-cd3c-4c62-8b24-e5ce1322bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out coefficents see weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f86a57-fd5f-44e0-b418-fa63fb9e2c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y_test_pred, y_train_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test_pred, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e317e017-ec14-4f78-b472-a23a94c3813f",
   "metadata": {},
   "source": [
    "## FEATURE SELECTION WITH KFOLD RFE GRIDSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e238085-2b53-4731-93dc-f02fac6aa940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    }
   ],
   "source": [
    "feature_count = len(X_train[1])\n",
    "folds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "hyper_params = [{'n_features_to_select': list(range(1, feature_count +1))}]\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "rfe = RFE(lm)             \n",
    "\n",
    "model_cv = GridSearchCV(estimator = rfe, \n",
    "                        param_grid = hyper_params, \n",
    "                        scoring= 'r2', \n",
    "                        cv = folds, \n",
    "                        verbose = 1,\n",
    "                        return_train_score=True)      \n",
    "\n",
    "model_cv.fit(X_train, y_train)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "014bf327-fe6c-461e-a025-836a275b4331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05152752, 0.04900518, 0.04915364, 0.05022837, 0.05064411])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results\n",
    "\n",
    "# plotting cv results\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n",
    "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
    "plt.xlabel('number of features')\n",
    "plt.ylabel('r-squared')\n",
    "plt.title(\"Optimal Number of Features\")\n",
    "plt.legend(['test score', 'train score'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683574fc-1cc2-43ab-b996-05d23f82d929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e158ab-deeb-437e-a42b-3205ec884173",
   "metadata": {},
   "outputs": [],
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ea1f1d9-405a-4c4a-b0e3-bbaeba74b050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05379055551551293\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_features_optimal = 10\n",
    "\n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train, y_train)\n",
    "\n",
    "rfe = RFE(lm, n_features_to_select=n_features_optimal)             \n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "\n",
    "# predict prices of X_test\n",
    "y_pred = lm.predict(X_test)\n",
    "r2 = sklearn.metrics.r2_score(y_test, y_pred)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514a460-c97f-43d1-9f5d-72f4be07e704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baad1804-21e8-4a09-ad09-19aaae376a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If diverge can use lasso or ridge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ccd522-7887-45c9-a21e-f1063e40ca69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf9d742d-32c3-4b71-8038-c5989a136b37",
   "metadata": {},
   "source": [
    "## PCA and loading plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdefaa60-faee-4974-ae35-ccbc7265c640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8d066-2aaa-4078-9809-76a2351313d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_new\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(X)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (3,3)\n",
    "fig = px.scatter(\n",
    "    components, x=0, y=1, color=c_y_km,\n",
    "    title=f'PC1 vs PC2 - kmeans',\n",
    "    labels={'0': 'PC 1', '1': 'PC 2'}\n",
    ")\n",
    "# from matplotlib.pylab import rcParams\n",
    "# rcParams['figure.figsize'] = 12, 10\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de59d33-3e2a-42fd-abec-c7d9552a8153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING PLOT\n",
    "\n",
    "X =  data_new\n",
    "features = X.columns\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "df_loading = pd.DataFrame(loadings, columns=('PC1','PC2'), index = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c1f5ae-7ff8-45f5-8718-7fdc70760895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PC1\n",
    "df_loading_abs = df_loading.abs().sort_values(by = 'PC1', ascending = False).head(10)\n",
    "\n",
    "df_loading_T = df_loading_abs.T\n",
    "\n",
    "labels = df_loading_T.columns\n",
    "PC1 = df_loading_T.loc['PC1']\n",
    "PC2 = df_loading_T.loc['PC2']\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "rects1 = ax.bar(x - width/2, PC1, width, label='PC1')\n",
    "rects2 = ax.bar(x + width/2, PC2, width, label='PC2')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Loading')\n",
    "ax.set_title('Loading Plot for PC1 and PC2')\n",
    "ax.set_xticks(x)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c206b-a654-4dc8-9c0b-4a357bef3794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot PC2\n",
    "df_loading_abs = df_loading.abs().sort_values(by = 'PC2', ascending = False).head(10)\n",
    "df_loading_T = df_loading_abs.T\n",
    "\n",
    "labels = df_loading_T.columns\n",
    "PC1 = df_loading_T.loc['PC1']\n",
    "PC2 = df_loading_T.loc['PC2']\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 6))\n",
    "rects1 = ax.bar(x - width/2, PC1, width, label='PC1')\n",
    "rects2 = ax.bar(x + width/2, PC2, width, label='PC2')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Loading')\n",
    "ax.set_title('Loading Plot for PC1 and PC2')\n",
    "ax.set_xticks(x)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "ax.bar_label(rects1, padding=3)\n",
    "ax.bar_label(rects2, padding=3)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
