{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d762ca93-c9bb-4d0a-8e0c-3d60c23cfcd6",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c6b2d1e8-7fcd-41df-8279-7ad7a6081d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import modules.help_functions as hf\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dbb17ee8-0353-44f0-8975-d72d4dd314d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file\n",
    "df = pd.read_csv('../data/flights.csv')\n",
    "df_delays = hf.get_avg_delay(df, ['carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "56c2ec26-f7f1-4d14-9f67-28182d3bbf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_testtrain_data_to_test_format(df):\n",
    "    \"\"\" Convert our testing data to be in the same format as the data to test (drop columns and reformat date)\"\"\"\n",
    "    \n",
    "    #convert date to datetime with 0's\n",
    "    df.fl_date = (df.fl_date + ' 00:00:00')\n",
    "    pd.to_datetime(df['fl_date'])\n",
    "    \n",
    "    #drop columns not present in test format\n",
    "    df.drop(columns=['dep_time',\n",
    "       'dep_delay', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled',\n",
    "       'cancellation_code', 'diverted', 'actual_elapsed_time', 'air_time', \n",
    "       'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay',\n",
    "       'late_aircraft_delay', 'first_dep_time', 'total_add_gtime',\n",
    "       'longest_add_gtime','no_name'], inplace = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f530f145-393c-4c98-b130-b111f684dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_from_test_format_to_fit_predict_format(df):\n",
    "    \"\"\"Adds in columns for model fitting and converts to numeric/ encoded categorical for ML model\"\"\"\n",
    "    \n",
    "    # Split crs_arr_time and crs_dep_time into hour of day (local)\n",
    "    df = hf.split_time_of_day_departure(df)\n",
    "    df = hf.split_time_of_day_arrival(df)\n",
    "    df.drop(columns=['crs_dep_time', 'crs_arr_time'], inplace=True)\n",
    "    \n",
    "    #take log distance\n",
    "    df['distance_log'] = np.log(df.distance)\n",
    "    df.drop(columns=['distance'], inplace=True)\n",
    "    \n",
    "    # encode hour of day departure\n",
    "    f = lambda x: str(int(np.floor(x)))\n",
    "    df['dep_hour'] = df['dep_hour'].apply(f)\n",
    "    df['arr_hour'] = df['arr_hour'].apply(f)\n",
    "    df = hf.encode_and_bind(df, 'dep_hour')\n",
    "    df = hf.encode_and_bind(df, 'arr_hour')\n",
    " \n",
    "    # Convert fl_date into day of week  # NOTE MAY WANT TO ADD BACK IN MONTH OR JAN 1 days\n",
    "    df = hf.add_weekday(df)\n",
    "    df.drop(columns=['fl_date'], inplace=True)\n",
    "    df = hf.encode_and_bind(df, 'weekday')\n",
    "    \n",
    "    # Add average delays to dest\n",
    "    df = df.merge(df_delays, on='dest', how='left')\n",
    "    \n",
    "    # Split city and state \n",
    "    hf.split_origin_city_state(df)\n",
    "    hf.split_dest_city_state(df)\n",
    "    df.drop(columns=['dest_city_name', 'origin_city_name'], inplace=True)\n",
    "    \n",
    "    # Encode top 10 cities in terms of traffic\n",
    "#     df = hf.make_col_value_qbins(df, 'origin_city', 'origin_city_bin', 10)\n",
    "#     df = hf.make_col_value_qbins(df, 'dest_city', 'dest_city_bin', 10)\n",
    "    \n",
    "    city_list = ['Chicago','Atlanta','New York','Dallas/Fort Worth','Denver','Charlotte','Houston','Washington','Los Angeles','Seattle']    \n",
    "    df.dest_city = np.where(df.dest_city.isin(city_list),df.dest_city, '0')\n",
    "    df.origin_city = np.where(df.origin_city.isin(city_list),df.origin_city, '0')\n",
    "    df = hf.encode_and_bind(df, 'origin_city')\n",
    "    df = hf.encode_and_bind(df, 'dest_city')\n",
    "    df.drop(columns=['dest_city', 'origin_city'], inplace=True)\n",
    "    \n",
    "    #Top 20 airport codes\n",
    "    # DOES AIRPORT TRAFFIC CORRELATED WITH ARR_DELAY?\n",
    "#     df = hf.make_col_value_qbins(df, 'origin', 'origin_bin', 15)\n",
    "#     df = hf.make_col_value_qbins(df, 'dest', 'dest_bin', 15)\n",
    "\n",
    "    top20_airport_code = ['LAX', 'ORD', 'EWR', 'SFO', 'LGA', 'DFW', 'LAS', 'CLT', 'DEN',\n",
    "                      'PHL', 'IAH', 'SEA', 'ATL', 'PHX', 'MCO', 'DTW', 'SLC', 'BOS',\n",
    "                      'JFK', 'MSP']\n",
    "    df.dest = np.where(df.dest.isin(top20_airport_code),df.dest, '0')\n",
    "    df.origin = np.where(df.origin.isin(top20_airport_code),df.origin, '0')\n",
    "    df = hf.encode_and_bind(df, 'dest')\n",
    "    df = hf.encode_and_bind(df, 'origin')\n",
    "    \n",
    "    #REMOVE negative targets (arr_delay - set to zero)\n",
    "#     df.arr_delay = np.where(df.arr_delay >0,df.arr_delay, 0)\n",
    "    \n",
    "    # State - Encode (based on # flights)\n",
    "    # DOES NUMBER OF FLIGHTS IN STATE CORRELATED WITH ARR_DELAY?\n",
    "    state_list = ['CA','TX', 'FL', 'IL', 'NY', 'GA', 'NC', 'CO', 'PA', 'WA']\n",
    "\n",
    "#     df = hf.make_col_value_qbins(df, 'origin_state', 'origin_state_bin', 15)\n",
    "#     df = hf.make_col_value_qbins(df, 'dest_state', 'dest_state_bin', 15)\n",
    "    df.dest_state = np.where(df.dest_state.isin(state_list),df.dest_state, '0')\n",
    "    df.origin_state = np.where(df.origin_state.isin(state_list),df.origin_state, '0')\n",
    "    df = hf.encode_and_bind(df, 'origin_state')\n",
    "    df = hf.encode_and_bind(df, 'dest_state')\n",
    "    df.drop(columns=['dest_state', 'origin_state'], inplace=True)\n",
    "          \n",
    "    # Convert Airline Carrier - Encode \n",
    "    df = hf.encode_and_bind(df, 'mkt_unique_carrier')\n",
    "    df.drop(columns = ['mkt_unique_carrier'], inplace=True)\n",
    "\n",
    "    # Origin Airport - Encode top 10 (rest in 'other') OR BIN according to passenger or flight volume\n",
    "    df = hf.make_col_value_bins(df, 'origin', 'origin_airport_fl_amt_bin', 7) \n",
    "    \n",
    "    \n",
    "    # Dest Airport - Encode top 10 or bin according to passenger of flight volume \n",
    "    df = hf.make_col_value_bins(df, 'dest', 'dest_airport_fl_amt_bin', 7) \n",
    "   \n",
    "    # Flight number ??? # drop for now? \n",
    "    df.drop(columns = ['mkt_carrier_fl_num'], inplace=True)\n",
    "    \n",
    "    # crs_elapsed # USE LONG HAUL SHORT HAUL\n",
    "    df['log_crs_elapsed_time'] = np.log(df.crs_elapsed_time)\n",
    "    df = hf.make_bin_column(df, 'log_crs_elapsed_time', 20) # 8-2/0.3\n",
    "    df.drop(columns = ['crs_elapsed_time','log_crs_elapsed_time' ], inplace=True)\n",
    "\n",
    "    # Drop rest\n",
    "    df.drop(columns=['branded_code_share', 'mkt_carrier','op_unique_carrier', 'tail_num', \n",
    "                     'op_carrier_fl_num', 'dep_hour', 'origin_airport_id', 'arr_hour','dest_airport_id',  'dup', 'flights'], errors='ignore', inplace = True)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1d9b6a22-8164-4eff-b766-25cabfb70fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocessed_data():\n",
    "    \"\"\" read in data file, convert to the format the given test data is in and add / format columns per feature engineering \"\"\"\n",
    "   \n",
    "    df = pd.read_csv('../data/flights.csv')\n",
    "    \n",
    "    # remove non-landing flights\n",
    "    df.dropna(subset=['arr_delay'], inplace=True)\n",
    "    \n",
    "#     remove outliers ### IS THIS GOING TO BACKFIRE?\n",
    "#     df_rm = df_rm[(np.abs(stats.zscore(df_rm['arr_delay'])) < 3)]\n",
    "    cols = ['arr_delay'] \n",
    "    Q1 = df[cols].quantile(0.25)\n",
    "    Q3 = df[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    \n",
    "    # convert\n",
    "    df = convert_testtrain_data_to_test_format(df)\n",
    "    df = convert_from_test_format_to_fit_predict_format(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "57802728-4cd6-4a5e-8bdd-893e53d5b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23285e9-5b67-45c6-b17d-51ae8161bbb5",
   "metadata": {},
   "source": [
    "### Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2a7e8fb6-fe7b-4d47-981a-70c3bab74812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ad6a6ab1-f54b-4d2d-b782-3f1846d42357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale the features\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# apply scaler() to all the numeric columns \n",
    "numeric_vars = ['avg_carrier_delay', \n",
    "                 'avg_weather_delay', \n",
    "                 'avg_nas_delay', \n",
    "                 'avg_security_delay', \n",
    "                 'avg_late_aircraft_delay', 'distance_log'\n",
    "                 ]\n",
    "df[numeric_vars] = scaler.fit_transform(df[numeric_vars])\n",
    "# list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb32053-5a65-4012-9be4-0ea143809e7d",
   "metadata": {},
   "source": [
    "### Remove Highly Correlated Features and small Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b11b98e7-84c2-490b-aad9-ab74ee3af0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    " def remove_small_variance(x, variance_threshold = 0.1):\n",
    "    # Assumptions - target variable removed, df is numeric\n",
    "    # import:\n",
    "    # from sklearn.feature_selection import VarianceThreshold\n",
    "    vt = VarianceThreshold(variance_threshold)\n",
    "    x_transformed = vt.fit_transform(x)\n",
    "    selected_columns = x.columns[vt.get_support()]\n",
    "    x_transformed = pd.DataFrame(x_transformed, columns = selected_columns)\n",
    "    return(x_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5e03a20d-8bc8-468b-b1a5-a12fa5c5e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(df, correlation_threshold=0.8):\n",
    "    #     Anything above correlation threshold will be tossed\n",
    "    # Assumptions - all numeric, target variable removed\n",
    "    # step 1\n",
    "    df_corr = df.corr().abs()\n",
    "\n",
    "    # step 2\n",
    "    indices = np.where(df_corr > correlation_threshold)\n",
    "    indices = [(df_corr.index[x], df_corr.columns[y])\n",
    "    for x, y in zip(*indices)\n",
    "        if x != y and x < y]\n",
    "\n",
    "    # step 3\n",
    "    for idx in indices: #each pair\n",
    "        try:\n",
    "            df.drop(idx[1], axis = 1, inplace=True)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b7822-5936-4e68-ac1e-d0ae755892ed",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a382e85f-7e09-4cc7-bf63-a3d5b80a46b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " r2: 0.07716835256104593\n",
      " MSE: 271.49542281669363\n",
      " MAE: 12.842297890060065\n",
      " model_fit 0.07716835256104593\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "y = df.arr_delay.to_numpy()\n",
    "X = df.drop(columns=['arr_delay']).to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=88)\n",
    "\n",
    "lr_baseline = LinearRegression()\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Save pickle file\n",
    "# model = lr_baseline\n",
    "# filename = '../model/linear_regression_all_features_except_neg_target.pkl'\n",
    "# pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "y_pred = lr_baseline.predict(X_test)\n",
    "\n",
    "r2_baseline = r2_score(y_test, y_pred)\n",
    "MSE_baseline = mean_squared_error(y_test,y_pred) \n",
    "RMSE_baseline = mean_squared_error(y_test,y_pred,squared=False)\n",
    "MAE_baseline = mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "print(f' r2: {r2_baseline}\\n MSE: {MSE_baseline}\\n MAE: {MAE_baseline}\\n model_fit {r2_baseline}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfcda46",
   "metadata": {},
   "source": [
    "### Polynomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6e238085-2b53-4731-93dc-f02fac6aa940",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 25.6 GiB for an array with shape (257219, 13366) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-863da81e0170>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpolyreg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPolynomialFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mpolyreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mr2_baseline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bootcamp\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bootcamp\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    666\u001b[0m             \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m         X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n\u001b[0m\u001b[0;32m    669\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bootcamp\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36m_preprocess_data\u001b[1;34m(X, y, fit_intercept, normalize, copy, sample_weight, return_mean, check_input)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\bootcamp\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmay_share_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray_orig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 813\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 25.6 GiB for an array with shape (257219, 13366) and data type float64"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# y = df.arr_delay.to_numpy()\n",
    "# X = df.drop(columns=['arr_delay']).to_numpy()\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=88)\n",
    "\n",
    "# degree=2\n",
    "# polyreg=make_pipeline(PolynomialFeatures(degree),LinearRegression())\n",
    "# polyreg.fit(X_train,y_train)\n",
    "\n",
    "# r2_baseline = r2_score(y_test, y_pred)\n",
    "# MSE_baseline = mean_squared_error(y_test,y_pred) \n",
    "# RMSE_baseline = mean_squared_error(y_test,y_pred,squared=False)\n",
    "# MAE_baseline = mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "# print(f' r2: {r2_baseline}\\n MSE: {MSE_baseline}\\n MAE: {MAE_baseline}\\n model_fit {r2_baseline}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd1cc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "bootcamp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
