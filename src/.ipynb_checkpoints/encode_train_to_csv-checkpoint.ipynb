{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370e4a25-47e4-48b1-933d-1dd809a20bcd",
   "metadata": {},
   "source": [
    "## Encode Data To Model\n",
    "This is a cumulation of our data analysis and feature engineering.\n",
    "* Used to generate a .csv file that can be used directly to split for model testing.\n",
    "* Data we gathered from sql is striped of extra columns and formated in the way that the testing data was given so that we can replicate the steps when given the testing data. \n",
    "* The various features were tweaked, added, one hot encoded, binned and dropped as a result of testing on a baseline linear regression model and feature analysis. \n",
    "* Lastly the encoded data is saved as a .csv and stored in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c6b2d1e8-7fcd-41df-8279-7ad7a6081d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import modules.help_functions as hf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5ba7cdd-fde7-4e21-b84e-67f168241ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lilakelland/Desktop/lighthouse_lab_midterm_project/src'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dbb17ee8-0353-44f0-8975-d72d4dd314d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in files\n",
    "df = pd.read_csv('../data/flights.csv')\n",
    "df_dep_delays = hf.get_avg_dep_delay(df, ['dep_delay'])\n",
    "df_dest_delays = hf.get_avg_dest_delay(df, ['carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "56c2ec26-f7f1-4d14-9f67-28182d3bbf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_format_to_test_data(df):\n",
    "    \"\"\" Convert our testing data to be in the same format as the data to test (drop columns and reformat date)\"\"\"\n",
    "    \n",
    "    #convert date to datetime with 0's\n",
    "    df.fl_date = (df.fl_date + ' 00:00:00')\n",
    "    pd.to_datetime(df['fl_date'])\n",
    "    \n",
    "    #drop columns not present in test format\n",
    "    df.drop(columns=['dep_time',\n",
    "       'dep_delay', 'taxi_out', 'wheels_off', 'wheels_on', 'taxi_in', 'arr_time', 'cancelled',\n",
    "       'cancellation_code', 'diverted', 'actual_elapsed_time', 'air_time', \n",
    "       'carrier_delay', 'weather_delay', 'nas_delay', 'security_delay',\n",
    "       'late_aircraft_delay', 'first_dep_time', 'total_add_gtime',\n",
    "       'longest_add_gtime','no_name'], inplace = True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f530f145-393c-4c98-b130-b111f684dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_engineering(df):\n",
    "    \"\"\"Adds in columns for model fitting and converts to numeric/ encoded categorical for ML model\"\"\"\n",
    "    \n",
    "    # Split crs_arr_time and crs_dep_time into hour of day (local)\n",
    "    df = hf.split_time_of_day_departure(df)\n",
    "    df = hf.split_time_of_day_arrival(df)\n",
    "    df.drop(columns=['crs_dep_time', 'crs_arr_time'], inplace=True)\n",
    "    \n",
    "    # encode hour of day departure\n",
    "    f = lambda x: str(int(np.floor(x)))\n",
    "\n",
    "    df['dep_hour'] = df['dep_hour'].apply(f)\n",
    "    df['arr_hour'] = df['arr_hour'].apply(f)\n",
    "    df.rename({'dep_hour_enc': 'dep_hour', 'arr_hour_enc': 'arr_hour'})\n",
    "    df = hf.encode_and_bind(df, 'dep_hour')\n",
    "    df = hf.encode_and_bind(df, 'arr_hour')\n",
    " \n",
    "    # Convert fl_date into day of week  \n",
    "    df = hf.add_weekday(df)\n",
    "#     df.drop(columns=['fl_date'], inplace=True) # drop at end\n",
    "    df = hf.make_categorical(df, ['weekday'])#######################################v7 add this in\n",
    "    df = hf.encode_and_bind(df, 'weekday')\n",
    "    df.drop(columns=['weekday'], inplace=True)##################################v7 add this in\n",
    "    \n",
    "    # Add average delays (weather, carrier, NAS, departure)\n",
    "    df = df.merge(df_dest_delays, on='dest', how='left')\n",
    "    df = df.merge(df_dep_delays, on='origin', how='left')\n",
    "    \n",
    "    # Split city and state \n",
    "    hf.split_origin_city_state(df)\n",
    "    hf.split_dest_city_state(df)\n",
    "    df.drop(columns=['dest_city_name', 'origin_city_name'], inplace=True)\n",
    "    \n",
    "    # Encode top 10 cities in terms of traffic\n",
    "    city_list = ['Chicago','Atlanta','New York','Dallas/Fort Worth','Denver','Charlotte','Houston','Washington','Los Angeles','Seattle']    \n",
    "    df.dest_city = np.where(df.dest_city.isin(city_list),df.dest_city, '0')\n",
    "    df.origin_city = np.where(df.origin_city.isin(city_list),df.origin_city, '0')\n",
    "    df = hf.encode_and_bind(df, 'origin_city')\n",
    "    df = hf.encode_and_bind(df, 'dest_city')\n",
    "    df.drop(columns=['dest_city', 'origin_city'], inplace=True)\n",
    "    \n",
    "    #Top 20 airport codes - in terms of number of flights\n",
    "    top20_airport_code = ['LAX', 'ORD', 'EWR', 'SFO', 'LGA', 'DFW', 'LAS', 'CLT', 'DEN',\n",
    "                      'PHL', 'IAH', 'SEA', 'ATL', 'PHX', 'MCO', 'DTW', 'SLC', 'BOS',\n",
    "                      'JFK', 'MSP']\n",
    "    df.dest = np.where(df.dest.isin(top20_airport_code),df.dest, '0')\n",
    "    df.origin = np.where(df.origin.isin(top20_airport_code),df.origin, '0')\n",
    "    df = hf.encode_and_bind(df, 'dest')\n",
    "    df = hf.encode_and_bind(df, 'origin')\n",
    "    \n",
    "    # Remove negative targets (arr_delay - set to zero)  # kept this in here as it had an interesting effect - both MAE and R2 decreased\n",
    "#     df.arr_delay = np.where(df.arr_delay >0,df.arr_delay, 0)\n",
    "    \n",
    "    # State - Encode (based on # flights)\n",
    "    state_list = ['CA','TX', 'FL', 'IL', 'NY', 'GA', 'NC', 'CO', 'PA', 'WA']\n",
    "    df.dest_state = np.where(df.dest_state.isin(state_list),df.dest_state, '0')\n",
    "    df.origin_state = np.where(df.origin_state.isin(state_list),df.origin_state, '0')\n",
    "    df = hf.encode_and_bind(df, 'origin_state')\n",
    "    df = hf.encode_and_bind(df, 'dest_state')\n",
    "    df.drop(columns=['dest_state', 'origin_state'], inplace=True)\n",
    "          \n",
    "    # Convert Airline Carrier - Encode \n",
    "    df = hf.encode_and_bind(df, 'mkt_unique_carrier')\n",
    "    df.drop(columns = ['mkt_unique_carrier'], inplace=True)\n",
    "\n",
    "    # Origin Airport Busyness - Encode top 10 (rest in 'other') OR BIN according to passenger or flight volume\n",
    "    df = hf.make_col_value_bins(df, 'origin', 'origin_airport_fl_amt_bin', 7) \n",
    "    \n",
    "    # Dest Airport Busyness- Encode top 10 or bin according to passenger of flight volume \n",
    "    df = hf.make_col_value_bins(df, 'dest', 'dest_airport_fl_amt_bin', 7) \n",
    "   \n",
    "    # Flight number\n",
    "    df_fl_num_delay = hf.fl_arr_delay(df)\n",
    "    df = df.merge(df_fl_num_delay, on=\"mkt_carrier_fl_num\", how='left')\n",
    "    # deal with nulls\n",
    "    arr_delay_all_flights_median = -8.0 # this is median of whole sample\n",
    "    df.fl_num_delay.fillna(arr_delay_all_flights_median, inplace=True)\n",
    "    df.drop(columns = ['mkt_carrier_fl_num'], inplace=True)\n",
    "    \n",
    "    # crs_elapsed # USE LONG HAUL SHORT HAUL\n",
    "    df['log_crs_elapsed_time'] = np.log(df.crs_elapsed_time)\n",
    "    df = hf.make_bin_column(df, 'log_crs_elapsed_time', 20) \n",
    "    df = hf.make_categorical(df, ['log_crs_elapsed_time_bin'])\n",
    "    df = hf.encode_and_bind(df, 'log_crs_elapsed_time_bin')\n",
    "    df.drop(columns = ['crs_elapsed_time','log_crs_elapsed_time', 'log_crs_elapsed_time_bin' ], inplace=True)\n",
    "\n",
    "    # Drop rest\n",
    "    df.drop(columns=['fl_date','branded_code_share', 'mkt_carrier','op_unique_carrier', 'tail_num', 'op_carrier_fl_num',\n",
    "                      'origin_airport_id', 'dest_airport_id', 'dep_hour','arr_hour', 'dup', 'flights'], errors='ignore', inplace = True)\n",
    "\n",
    "    # Drop features from LDA #_________________________________fix for V7\n",
    "    df.drop(columns=['origin_airport_fl_amt_bin', 'dest_airport_fl_amt_bin', \n",
    "                'dest_0', \n",
    "                'weekday_0', 'weekday_1', 'weekday_2', \n",
    "                'weekday_3', 'weekday_4', \n",
    "                'weekday_5', 'weekday_6', \n",
    "                'origin_city_0', \n",
    "                'origin_state_0'], errors =False, inplace = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1d9b6a22-8164-4eff-b766-25cabfb70fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_na_outliers(df):\n",
    "    \"\"\" read data, convert to the format the given test data is in and add / format columns per feature engineering \"\"\"\n",
    "    # remove non-landing flights\n",
    "    df.dropna(subset=['arr_delay'], inplace=True)\n",
    "    \n",
    "    # remove outliers \n",
    "    cols = ['arr_delay'] \n",
    "    Q1 = df[cols].quantile(0.25)\n",
    "    Q3 = df[cols].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df = df[~((df[cols] < (Q1 - 1.5 * IQR)) |(df[cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad6a6ab1-f54b-4d2d-b782-3f1846d42357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(df):\n",
    "#     scaler = MinMaxScaler()\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # apply scaler() to all the numeric columns \n",
    "    numeric_vars = ['avg_carrier_delay', \n",
    "                     'avg_weather_delay', \n",
    "                     'avg_nas_delay', \n",
    "                     'avg_security_delay', \n",
    "                     'avg_late_aircraft_delay', 'avg_dep_delay', 'distance', 'fl_num_delay' #,'op_carrier_fl_num'\n",
    "                     ]\n",
    "    df[numeric_vars] = scaler.fit_transform(df[numeric_vars])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b11b98e7-84c2-490b-aad9-ab74ee3af0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOUND THAT THIS HAD NEGATIVE IMPACT AND NO LONGER USING\n",
    "#  def remove_small_variance(x, variance_threshold = 0.1):\n",
    "#     # Assumptions - target variable removed, df is numeric\n",
    "\n",
    "#     vt = VarianceThreshold(variance_threshold)\n",
    "#     x_transformed = vt.fit_transform(x)\n",
    "#     selected_columns = x.columns[vt.get_support()]\n",
    "#     x_transformed = pd.DataFrame(x_transformed, columns = selected_columns)\n",
    "#     return(x_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e03a20d-8bc8-468b-b1a5-a12fa5c5e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(df, correlation_threshold=0.8):\n",
    "    \"\"\" Any pair above the correlation threshold, one feature will be removed \"\"\"\n",
    "    \n",
    "    df_corr = df.corr().abs()\n",
    "\n",
    "    indices = np.where(df_corr > correlation_threshold)\n",
    "    indices = [(df_corr.index[x], df_corr.columns[y])\n",
    "    for x, y in zip(*indices)\n",
    "        if x != y and x < y]\n",
    "\n",
    "    for idx in indices: #each pair\n",
    "        try:\n",
    "            df.drop(idx[1], axis = 1, inplace=True)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0b9717b0-5c5b-4b5e-862b-40e074e27b5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['dest_0' 'weekday_0' 'origin_city_0' 'origin_state_0'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-504ce02efd55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_format_to_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_feature_engineering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# df = data_to_model_format(df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_na_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-458fd4b31dc6>\u001b[0m in \u001b[0;36madd_feature_engineering\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# Drop features from LDA #_________________________________fix for V7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     df.drop(columns=['origin_airport_fl_amt_bin', \n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;34m'dest_airport_fl_amt_bin'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;34m'dest_0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bootcamp_env/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bootcamp_env/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4904\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4905\u001b[0m         \"\"\"\n\u001b[0;32m-> 4906\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   4907\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4908\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bootcamp_env/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4150\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bootcamp_env/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   4183\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4184\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4185\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4186\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/bootcamp_env/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6015\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6017\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6019\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['dest_0' 'weekday_0' 'origin_city_0' 'origin_state_0'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Transform df to encoded training data\n",
    "df = pd.read_csv('../data/flights.csv')\n",
    "df_dep_delays = hf.get_avg_dep_delay(df, ['dep_delay'])\n",
    "df_dest_delays = hf.get_avg_dest_delay(df, ['carrier_delay', 'weather_delay', 'nas_delay', 'security_delay', 'late_aircraft_delay'])\n",
    "\n",
    "df = change_format_to_test_data(df)\n",
    "df = add_feature_engineering(df)\n",
    "# df = data_to_model_format(df)\n",
    "df = remove_na_outliers(df)\n",
    "df = scale(df)\n",
    "df = remove_highly_correlated_features(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "498ed1be-239d-46f3-8629-0829abf7426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to data folder\n",
    "df.to_csv(\"../data/encoded_training_data_v7.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f6bb8786-ca75-4b5a-8a08-25ebfabd08ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('float64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('float64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64'),\n",
       " dtype('int64')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df1 = pd.read_csv(\"../data/encoded_training_data_v6.csv\")\n",
    "# df1\n",
    "# df[df.dtypes == 'O']\n",
    "list(df1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b7822-5936-4e68-ac1e-d0ae755892ed",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a382e85f-7e09-4cc7-bf63-a3d5b80a46b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " r2: 0.07630527757520533\n",
      " MSE: 271.74933793637297\n",
      " MAE: 12.848942878135732\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "y = df.arr_delay.to_numpy()\n",
    "X = df.drop(columns=['arr_delay']).to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=88)\n",
    "\n",
    "lr_baseline = LinearRegression()\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Save pickle file\n",
    "# model = lr_baseline\n",
    "# filename = '../model/linear_regression_all_features_except_neg_target.pkl'\n",
    "# pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "y_pred = lr_baseline.predict(X_test)\n",
    "\n",
    "r2_baseline = r2_score(y_test, y_pred)\n",
    "MSE_baseline = mean_squared_error(y_test,y_pred) \n",
    "RMSE_baseline = mean_squared_error(y_test,y_pred,squared=False)\n",
    "MAE_baseline = mean_absolute_error(y_test,y_pred)\n",
    "\n",
    "print(f' r2: {r2_baseline}\\n MSE: {MSE_baseline}\\n MAE: {MAE_baseline}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d2afef",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2: 0.07581815644074663\n",
    " MSE: 271.89264810429995\n",
    " MAE: 12.85399770731786\n",
    "\n",
    "r2: 0.07815832328035022\n",
    " MSE: 271.2041751988215\n",
    " MAE: 12.83497665252519\n",
    "\n",
    "#  r2: 0.08013667354836995\n",
    "#  MSE: 270.622148082623\n",
    "#  MAE: 12.81701225015653"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
